{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8d5f99-d7ee-4c54-96df-95943efcb680",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717e23b-a89e-4923-9a8c-ac551813a600",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example: Vandermonde matrix\n",
    "\n",
    "$$ \\begin{align}V=\\begin{bmatrix}1&\\alpha _{1}&\\alpha _{1}^{2}&\\dots &\\alpha _{1}^{n-1}\\\\1&\\alpha _{2}&\\alpha _{2}^{2}&\\dots &\\alpha _{2}^{n-1}\\\\1&\\alpha _{3}&\\alpha _{3}^{2}&\\dots &\\alpha _{3}^{n-1}\\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\1&\\alpha _{m}&\\alpha _{m}^{2}&\\dots &\\alpha _{m}^{n-1}\\end{bmatrix}\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d08891-8da2-4d03-8295-93ff62340981",
   "metadata": {},
   "source": [
    "### Python / numpy\n",
    "\n",
    "<p><img src=\"../images/numpy_vander.png\" alt=\"drawing\" width=\"1500\"/></p>\n",
    "\n",
    "(The source code for this function is [here](https://github.com/numpy/numpy/blob/v1.16.1/numpy/lib/twodim_base.py#L475-L563). It calls `np.multiply.accumulate` which is implemented in C [here](https://github.com/numpy/numpy/blob/deea4983aedfa96905bbaee64e3d1de84144303f/numpy/core/src/umath/ufunc_object.c#L3678). However, this code doesn't actually perform the computation, it basically only checks types and stuff. The actual kernel that gets called is [here](https://github.com/numpy/numpy/blob/deea4983aedfa96905bbaee64e3d1de84144303f/numpy/core/src/umath/loops.c.src#L1742). This isn't even C code but a template for C code which is used to generate type specific kernels.)\n",
    "\n",
    "**Overall, this setup only supports a limited set of types, like `Float64`, `Float32`, and so forth.**\n",
    "\n",
    "\n",
    "### Julia\n",
    "\n",
    "Here is a simple generic Julia implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vander(x::AbstractVector{T}) where T\n",
    "    m = length(x)\n",
    "    V = Matrix{T}(undef, m, m)\n",
    "    for j = 1:m\n",
    "        V[j,1] = one(x[j])\n",
    "    end\n",
    "    for i= 2:m\n",
    "        for j = 1:m\n",
    "            V[j,i] = x[j] * V[j,i-1]\n",
    "            end\n",
    "        end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad55da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vander(1:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ba246-eefc-484f-8873-094c8b0e1dc2",
   "metadata": {},
   "source": [
    "#### Quick speed comparison\n",
    "\n",
    "<img src=\"../images/vandermonde.svg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Note that the clean and concise Julia implementation is **beating numpy's C implementation for small matrices** and is **on-par for large matrix sizes**.\n",
    "\n",
    "At the same time, **the Julia code is *generic* and works for arbitrary types!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vander(Int32[4, 8, 16, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256ae63-8eb9-4d89-9f1d-414627f05589",
   "metadata": {},
   "source": [
    "It even works for non-numerical types. The only requirement is that the type has a *one* (identity element) and a multiplication operation defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8ede1-440e-4bdc-a742-8550d0ae1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vander([\"This\", \"also\", \"works\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f14a8c-b56c-4d58-aff9-8e617751b7ee",
   "metadata": {},
   "source": [
    "### Emergent feature: Symbolic computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919a579-b296-4c4d-b219-d7e3f6ba1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Symbolics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737fc3b-8646-40ee-98a2-5d44e3993235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@variables a b c d e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be6e40-a389-49dc-a09c-d3f00b2808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vander([a,b,c,d,e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adaa59-727b-40d0-9cc4-71c8c91c99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute(v, Dict(b => 2, d => 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219354ea-e66e-49f0-8e55-3d72591e47c4",
   "metadata": {},
   "source": [
    "## Emergent feature: Differential equation solving with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374888b-23ff-4adb-9d1f-2d85ff6cbafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using OrdinaryDiffEq, Measurements, Plots\n",
    "\n",
    "#Half-life of Carbon-14 is 5730 years.\n",
    "c = 5.730 ± 2\n",
    "\n",
    "#Setup\n",
    "u0 = 1.0 ± 0.1\n",
    "tspan = (0.0, 1.0)\n",
    "\n",
    "#Define the problem\n",
    "radioactivedecay(u,p,t) = -c*u\n",
    "\n",
    "#Pass to solver\n",
    "prob = ODEProblem(radioactivedecay,u0,tspan)\n",
    "sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8);\n",
    "\n",
    "plot(sol.t, sol.u, ylabel=\"u(t)\", xlabel=\"t\", lw=2, legend=false, frame=:box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c878ccc-1b85-43a5-83e6-9fa3acf748c0",
   "metadata": {},
   "source": [
    "**Historical note**: In some sense, **Julia implemented that feature by itself**. The authors of Measurements.jl and DifferentialEquations.jl [never had any collabration on this](https://discourse.julialang.org/t/differentialequations-jl-and-measurements-jl/6350)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efcbb6",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e40bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics, ProgressMeter\n",
    "\n",
    "device = gpu_device()\n",
    "\n",
    "# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\n",
    "noisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\n",
    "truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n",
    "\n",
    "# Define our model, a multi-layer perceptron with one hidden layer of size 3:\n",
    "model = Chain(\n",
    "    Dense(2 => 3, tanh),      # activation function inside layer\n",
    "    BatchNorm(3),\n",
    "    Dense(3 => 2)\n",
    ") |> device  # move model to GPU, if one is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model encapsulates parameters, randomly initialised. Its initial output is:\n",
    "out1 = model(noisy |> device)    # 2×1000 Matrix{Float32}, or CuArray{Float32}\n",
    "probs1 = softmax(out1) |> cpu    # normalise to get probabilities (and move off GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdea96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the model, we use batches of 64 samples, and one-hot encoding:\n",
    "target = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\n",
    "loader = Flux.DataLoader((noisy, target), batchsize=64, shuffle=true);\n",
    "opt_state = Flux.setup(Flux.Adam(0.01), model);  # will store optimiser momentum, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c66a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop, using the whole data set 1000 times:\n",
    "losses = []\n",
    "@showprogress for epoch in 1:1_000\n",
    "    for xy_cpu in loader\n",
    "        # Unpack batch of data, and move to GPU:\n",
    "        x, y = xy_cpu |> device\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = m(x)\n",
    "            Flux.logitcrossentropy(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(opt_state, model, grads[1])\n",
    "        push!(losses, loss)  # logging, outside gradient context\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model(noisy |> device)         # first row is prob. of true, second row p(false)\n",
    "probs2 = softmax(out2) |> cpu         # normalise to get probabilities\n",
    "mean((probs2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots  # to draw the above figure\n",
    "\n",
    "p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\n",
    "p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=probs1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\n",
    "p_done = scatter(noisy[1,:], noisy[2,:], zcolor=probs2[1,:], title=\"Trained network\", legend=false)\n",
    "\n",
    "plot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "    yaxis=\"loss\", label=\"per batch\")\n",
    "n = length(loader)\n",
    "plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "    label=\"epoch mean\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390bbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
